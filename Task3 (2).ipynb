{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***Context-Aware Chatbot Using LangChain or RAG***"
      ],
      "metadata": {
        "id": "uOzSifIQgHVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add 'langchain-text-splitters' to your install list\n",
        "!pip install -q langchain langchain-community langchain-text-splitters chromadb wikipedia"
      ],
      "metadata": {
        "collapsed": true,
        "id": "l4uaoeu5jZnz"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U langchain-openai"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Sme4Bqw_o3T2"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U langchain-classic"
      ],
      "metadata": {
        "id": "D-2DCImOpbuH"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit\n",
        "!npm install -g localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8s98tIVYFcD",
        "outputId": "bb7c90e6-0f9b-4392-87b9-0fcd2d089dc1"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K\n",
            "changed 22 packages in 4s\n",
            "\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Import Libraries***"
      ],
      "metadata": {
        "id": "9xJo5MM9Er8w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "okZJHYTMcFid"
      },
      "outputs": [],
      "source": [
        "# to handle data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WikipediaLoader\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from langchain_classic.chains import ConversationalRetrievalChain\n",
        "from langchain_classic.memory import ConversationBufferMemory\n",
        "from langchain_classic.prompts import PromptTemplate\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "import streamlit as st\n",
        "\n",
        "\n",
        "# ignore warnings\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Load Content from Wikipedia***"
      ],
      "metadata": {
        "id": "d-X7Eu-nkfKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Search and load a specific topic from Wikipedia\n",
        "# load_max_docs=1 ensures we get the most relevant page\n",
        "topic = \"Generative artificial intelligence\"\n",
        "loader = WikipediaLoader(query=topic, load_max_docs=2)\n",
        "docs = loader.load()\n",
        "\n",
        "print(f\"Loaded {len(docs)} pages from Wikipedia.\")\n",
        "print(f\"Source: {docs[0].metadata['source']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "907F9eYDj8n9",
        "outputId": "110eb4f7-dad9-491a-f9a7-f3d39a7dffcf"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 2 pages from Wikipedia.\n",
            "Source: https://en.wikipedia.org/wiki/Generative_artificial_intelligence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split text into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
        "chunks = text_splitter.split_documents(docs)\n",
        "\n"
      ],
      "metadata": {
        "id": "36YM4L6Yj_eL"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize HuggingFace Embeddings (Free and runs on Colab GPU/CPU)\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n"
      ],
      "metadata": {
        "id": "bLblDcqvmdkO"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Vector Store (Chroma)\n",
        "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings)\n",
        "print(\"Vector database created successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rP541pUYmwN9",
        "outputId": "a6968841-7dc0-48b4-f479-4bd0f1101008"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector database created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set your OpenRouter API Key\n",
        "os.environ[\"OPENROUTER_API_KEY\"] = userdata.get('OPENROUTER_API_KEY')\n"
      ],
      "metadata": {
        "id": "IMx8s5zPPLrp"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize using ChatOpenAI (the OpenRouter \"bridge\")\n",
        "llm = ChatOpenAI(\n",
        "    model=\"xiaomi/mimo-v2-flash:free\", # Note the \"google/\" prefix\n",
        "    openai_api_key=os.environ[\"OPENROUTER_API_KEY\"],\n",
        "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
        "    default_headers={\n",
        "        \"HTTP-Referer\": \"https://colab.research.google.com/\", # Required by OpenRouter\n",
        "        \"X-Title\": \"My LangChain RAG Bot\",\n",
        "    },\n",
        "    temperature=0.3\n",
        ")\n"
      ],
      "metadata": {
        "id": "nD6lZxLIPZIm"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Custom System Prompt\n",
        "# This ensures the bot only uses Wikipedia and doesn't guess.\n",
        "custom_template = \"\"\"\n",
        "You are a helpful AI assistant. Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Always provide a concise and professional response.\n",
        "\n",
        "Context: {context}\n",
        "Chat History: {chat_history}\n",
        "Question: {question}\n",
        "\n",
        "Helpful Answer:\"\"\"\n",
        "\n",
        "CUSTOM_PROMPT = PromptTemplate(\n",
        "    template=custom_template,\n",
        "    input_variables=[\"context\", \"chat_history\", \"question\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "JTPAAfKgqmCR"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Setup Memory\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True,\n",
        "    output_key='answer' # Important for Gemini chains\n",
        ")"
      ],
      "metadata": {
        "id": "xXMZUHciqp2l"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Build the Conversational Chain\n",
        "qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 5}), # Retrieve top 5 chunks\n",
        "    memory=memory,\n",
        "    combine_docs_chain_kwargs={\"prompt\": CUSTOM_PROMPT}\n",
        ")\n",
        "\n",
        "print(\"RAG Chatbot is live!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzWoR4e1qYms",
        "outputId": "fe949531-a1f1-4e67-872e-8a8f54126486"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG Chatbot is live!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_bot():\n",
        "    print(f\"--- Chatbot is ready! Topic: {topic} ---\")\n",
        "    print(\"(Type 'exit' to stop)\")\n",
        "\n",
        "    while True:\n",
        "        query = input(\"You: \")\n",
        "        if query.lower() in [\"exit\", \"quit\", \"stop\"]:\n",
        "            break\n",
        "\n",
        "        result = qa_chain.invoke({\"question\": query})\n",
        "        print(f\"Bot: {result['answer']}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "QAnWHJKaqtEs"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_with_bot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybSHjExPrBdi",
        "outputId": "658b2809-2e23-4013-8666-e7cb7c2acfb2"
      },
      "execution_count": 66,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Chatbot is ready! Topic: Generative artificial intelligence ---\n",
            "(Type 'exit' to stop)\n",
            "You: AI\n",
            "Bot: Generative artificial intelligence (Generative AI or GenAI) is a subfield of artificial intelligence that uses generative models to generate text, images, videos, audio, software code or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data in response to input, which often comes in the form of natural language prompts.\n",
            "\n",
            "You: stop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Streamlit***"
      ],
      "metadata": {
        "id": "DQ1p_HJjgtZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "# --- UI CONFIG ---\n",
        "st.title(\"ü¶ú Wikipedia RAG Bot\")\n",
        "\n",
        "# --- LOAD API KEY (Using Colab Secrets logic for Streamlit) ---\n",
        "# Note: In a real deployment, use st.secrets. In Colab, we can use os.environ\n",
        "api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
        "\n",
        "if not api_key:\n",
        "    st.error(\"API Key not found. Please run the environment setup cell first.\")\n",
        "    st.stop()\n",
        "\n",
        "# --- MODEL SETUP ---\n",
        "if \"llm\" not in st.session_state:\n",
        "    st.session_state.llm = ChatOpenAI(\n",
        "        model=\"google/gemini-2.0-flash\",\n",
        "        api_key=api_key,\n",
        "        base_url=\"https://openrouter.ai/api/v1\",\n",
        "        default_headers={\"HTTP-Referer\": \"https://colab.research.google.com/\"}\n",
        "    )\n",
        "\n",
        "# --- CHAT INTERFACE ---\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "for msg in st.session_state.messages:\n",
        "    st.chat_message(msg[\"role\"]).write(msg[\"content\"])\n",
        "\n",
        "if prompt := st.chat_input():\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    st.chat_message(\"user\").write(prompt)\n",
        "\n",
        "    # Simple placeholder logic (Connect your qa_chain here)\n",
        "    response = f\"I received: {prompt}. (Connect your vectorstore to get RAG answers!)\"\n",
        "\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "    st.chat_message(\"assistant\").write(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGoui5gXrDUo",
        "outputId": "1f7e9064-5333-4cb4-e40d-68a840643131"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "print(\"Password/Endpoint IP for localtunnel is:\", urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7fAw6ZsGYxRP",
        "outputId": "8361f28b-ad42-4412-95c5-cb849c4d6460"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Password/Endpoint IP for localtunnel is: 34.124.224.238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Kill any existing streamlit processes\n",
        "!pkill streamlit\n",
        "\n",
        "# 2. Run Streamlit with specific \"Headless\" and \"CORS\" flags\n",
        "!streamlit run app.py --server.headless true --server.enableCORS false --server.enableXsrfProtection false &>/content/logs.txt &\n",
        "\n",
        "# 3. Expose it again\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLrDifGQZKx5",
        "outputId": "a2a99c6d-f74c-4d5a-e076-ab89ba9135fd"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0Kyour url is: https://little-bushes-serve.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sUJSfauUkkTj"
      },
      "execution_count": 69,
      "outputs": []
    }
  ]
}